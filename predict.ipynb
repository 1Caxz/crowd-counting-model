{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14eb4f04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34-peoples.jpg: Estimated Count = 54.78\n",
      "271-peoples.jpg: Estimated Count = 175.04\n",
      "93-peoples.jpg: Estimated Count = 103.10\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms.functional import resize as tvf_resize\n",
    "from models.csrnet_mbv3 import MobileCSRNet\n",
    "\n",
    "# ==== Config ====\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "IMAGE_DIR = \"inference/test_images\"\n",
    "# MODEL_PATH = \"build/csrnet_vgg_B.pth\"\n",
    "MODEL_PATH = \"build/csrnet_mobile_B.pt\"\n",
    "SAVE_HEATMAP = True  # Set False jika tidak ingin simpan visualisasi density map\n",
    "HEATMAP_DIR = \"inference/heatmaps\"\n",
    "\n",
    "os.makedirs(HEATMAP_DIR, exist_ok=True)\n",
    "\n",
    "# ==== Transform ====\n",
    "# transform = transforms.Compose([\n",
    "#     # transforms.Resize((384, 384)),\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "#                          std=[0.229, 0.224, 0.225]),\n",
    "\n",
    "# ])\n",
    "\n",
    "m_transform = transforms.Compose([\n",
    "    transforms.Resize((512, 512)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                            [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# ==== Load Model ====\n",
    "model = MobileCSRNet().to(DEVICE)\n",
    "model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE, weights_only=False))\n",
    "# checkpoint = torch.load(MODEL_PATH, map_location=DEVICE, weights_only=False)\n",
    "# model.load_state_dict(checkpoint['state_dict'])\n",
    "# print(f\"MAE: {checkpoint['best_prec1']} - epoch: {checkpoint['epoch']}\")\n",
    "model.eval()\n",
    "\n",
    "# ==== Predict ====\n",
    "image_files = [f for f in os.listdir(\n",
    "    IMAGE_DIR) if f.endswith(('.jpg', '.png'))]\n",
    "\n",
    "for fname in image_files:\n",
    "    path = os.path.join(IMAGE_DIR, fname)\n",
    "    image = Image.open(path).convert('RGB')\n",
    "    input_img = m_transform(image).unsqueeze(0).to(DEVICE)  # [1, 3, 384, 384]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(input_img)  # [1, 1, H, W]\n",
    "        density_map = output.squeeze().cpu().numpy()\n",
    "        total_count = density_map.sum()\n",
    "\n",
    "    print(f\"{fname}: Estimated Count = {total_count:.2f}\")\n",
    "\n",
    "    # Optional: Save heatmap\n",
    "    if SAVE_HEATMAP:\n",
    "        # plt.figure(figsize=(4, 4))\n",
    "        plt.imshow(density_map, cmap='jet')\n",
    "        plt.axis('off')\n",
    "        plt.title(f'Count: {total_count:.1f}')\n",
    "        heatmap_path = os.path.join(\n",
    "            HEATMAP_DIR, fname.replace('.jpg', '_heatmap.png'))\n",
    "        plt.savefig(heatmap_path, bbox_inches='tight', pad_inches=0)\n",
    "        plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10e3866a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-09 14:23:07.305826: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-06-09 14:23:07.953597: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1749453788.096985   80301 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1749453788.135415   80301 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1749453788.471020   80301 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1749453788.471064   80301 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1749453788.471065   80301 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1749453788.471066   80301 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-06-09 14:23:08.503791: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/home/caxz/Documents/Git/crowd-counting-model/venv/lib/python3.12/site-packages/tensorflow/lite/python/interpreter.py:457: UserWarning:     Warning: tf.lite.Interpreter is deprecated and is scheduled for deletion in\n",
      "    TF 2.20. Please use the LiteRT interpreter from the ai_edge_litert package.\n",
      "    See the [migration guide](https://ai.google.dev/edge/litert/migration)\n",
      "    for details.\n",
      "    \n",
      "  warnings.warn(_INTERPRETER_DELETION_WARNING)\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34-peoples.jpg: Estimated Count = 53.48\n",
      "271-peoples.jpg: Estimated Count = 176.56\n",
      "93-peoples.jpg: Estimated Count = 101.65\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ==== Config ====\n",
    "IMAGE_DIR = \"inference/test_images\"\n",
    "TFLITE_MODEL_PATH = \"build/csrnet_mobile_B_float16.tflite\"\n",
    "HEATMAP_DIR = \"inference/heatmaps_tflite\"\n",
    "SAVE_HEATMAP = True\n",
    "\n",
    "os.makedirs(HEATMAP_DIR, exist_ok=True)\n",
    "\n",
    "# ==== Load TFLite Model ====\n",
    "interpreter = tf.lite.Interpreter(model_path=TFLITE_MODEL_PATH)\n",
    "interpreter.allocate_tensors()\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "input_shape = input_details[0]['shape']  # e.g., [1, 512, 512, 3] or [1, 3, 512, 512]\n",
    "\n",
    "is_channels_first = input_shape[1] == 3  # Check if input is NCHW or NHWC\n",
    "\n",
    "# ==== Transform Function ====\n",
    "def preprocess(image):\n",
    "    image = image.resize((512, 512))\n",
    "    image = np.array(image).astype(np.float32) / 255.0\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    image = (image - mean) / std\n",
    "    if is_channels_first:\n",
    "        image = np.transpose(image, (2, 0, 1))  # HWC -> CHW\n",
    "    return np.expand_dims(image, axis=0).astype(np.float32)\n",
    "\n",
    "# ==== Inference ====\n",
    "image_files = [f for f in os.listdir(IMAGE_DIR) if f.endswith(('.jpg', '.png'))]\n",
    "\n",
    "for fname in image_files:\n",
    "    path = os.path.join(IMAGE_DIR, fname)\n",
    "    image = Image.open(path).convert('RGB')\n",
    "    input_tensor = preprocess(image)\n",
    "\n",
    "    interpreter.set_tensor(input_details[0]['index'], input_tensor)\n",
    "    interpreter.invoke()\n",
    "    output = interpreter.get_tensor(output_details[0]['index'])  # [1, 1, 64, 64]\n",
    "\n",
    "    density_map = output.squeeze()\n",
    "    total_count = density_map.sum()\n",
    "\n",
    "    print(f\"{fname}: Estimated Count = {total_count:.2f}\")\n",
    "\n",
    "    if SAVE_HEATMAP:\n",
    "        plt.imshow(density_map, cmap='jet')\n",
    "        plt.axis('off')\n",
    "        plt.title(f'Count: {total_count:.1f}')\n",
    "        heatmap_path = os.path.join(HEATMAP_DIR, fname.replace('.jpg', '_heatmap.png'))\n",
    "        plt.savefig(heatmap_path, bbox_inches='tight', pad_inches=0)\n",
    "        plt.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
