{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75ca4f37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ TorchScript saved to build/csrnet_mobile_B_scripted.pt\n",
      "✅ ONNX model exported to build/csrnet_mobile_B.onnx\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from models.csrnet_mbv3 import MobileCSRNet\n",
    "import os\n",
    "\n",
    "# --- Load model ---\n",
    "model = MobileCSRNet()\n",
    "model.load_state_dict(torch.load(\"build/csrnet_mobile_B.pt\", map_location=torch.device('cpu')))\n",
    "model.eval()\n",
    "\n",
    "# --- Trace dummy input ---\n",
    "dummy_input = torch.randn(1, 3, 512, 512)\n",
    "traced_model = torch.jit.trace(model, dummy_input)\n",
    "torchscript_path = \"build/csrnet_mobile_B_scripted.pt\"\n",
    "traced_model.save(torchscript_path)\n",
    "print(f\"✅ TorchScript saved to {torchscript_path}\")\n",
    "\n",
    "# --- ONNX export (optional step before TFLite conversion) ---\n",
    "onnx_path = \"build/csrnet_mobile_B.onnx\"\n",
    "torch.onnx.export(model, dummy_input, onnx_path,\n",
    "                  input_names=['input'], output_names=['output'],\n",
    "                  dynamic_axes={'input': {0: 'batch'}, 'output': {0: 'batch'}},\n",
    "                  opset_version=11)\n",
    "print(f\"✅ ONNX model exported to {onnx_path}\")\n",
    "\n",
    "# Note: TFLite conversion must be done using ONNX → TF → TFLite via Python tools like tf2onnx + tf.lite\n",
    "# Example (to run separately):\n",
    "#   python -m tf2onnx.convert --onnx csrnet_mobile.onnx --saved-model tf_model\n",
    "#   then use TF to convert tf_model to .tflite\n",
    "\n",
    "# Tip: In Flutter, use \"tflite_flutter\" to run .tflite model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be5b3bfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpw1f_46tu/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpw1f_46tu/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at '/tmp/tmpw1f_46tu'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 512, 512, 3), dtype=tf.float32, name='keras_tensor_2')\n",
      "Output Type:\n",
      "  TensorSpec(shape=(None, 512, 512, 1), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  140274914854864: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140276966329808: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "✅ TFLite model saved as csrnet_mobile_B.tflite\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1749441864.982626 4064405 tf_tfl_flatbuffer_helpers.cc:365] Ignored output_format.\n",
      "W0000 00:00:1749441864.982653 4064405 tf_tfl_flatbuffer_helpers.cc:368] Ignored drop_control_dependency.\n",
      "2025-06-09 11:04:24.982781: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /tmp/tmpw1f_46tu\n",
      "2025-06-09 11:04:24.983039: I tensorflow/cc/saved_model/reader.cc:52] Reading meta graph with tags { serve }\n",
      "2025-06-09 11:04:24.983047: I tensorflow/cc/saved_model/reader.cc:147] Reading SavedModel debug info (if present) from: /tmp/tmpw1f_46tu\n",
      "2025-06-09 11:04:24.984775: I tensorflow/cc/saved_model/loader.cc:236] Restoring SavedModel bundle.\n",
      "2025-06-09 11:04:24.997231: I tensorflow/cc/saved_model/loader.cc:220] Running initialization op on SavedModel bundle at path: /tmp/tmpw1f_46tu\n",
      "2025-06-09 11:04:25.000469: I tensorflow/cc/saved_model/loader.cc:471] SavedModel load for tags { serve }; Status: success: OK. Took 17691 microseconds.\n"
     ]
    }
   ],
   "source": [
    "import onnx\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from onnxruntime import InferenceSession\n",
    "from tensorflow import keras\n",
    "\n",
    "# Load ONNX model\n",
    "onnx_model_path = \"build/csrnet_mobile_B.onnx\"\n",
    "session = InferenceSession(onnx_model_path)\n",
    "\n",
    "# Dummy input sesuai ONNX shape\n",
    "input_name = session.get_inputs()[0].name\n",
    "input_shape = session.get_inputs()[0].shape\n",
    "dummy_input = np.random.randn(1, 3, 512, 512).astype(np.float32)\n",
    "\n",
    "# Run ONNX model (untuk verifikasi)\n",
    "output = session.run(None, {input_name: dummy_input})\n",
    "\n",
    "# Bangun ulang model TensorFlow dengan hasil output ONNX\n",
    "# Ini mock karena kita tidak bisa langsung convert\n",
    "# Buat dummy model dengan signature yang sama\n",
    "inputs = tf.keras.Input(shape=(512, 512, 3))\n",
    "x = tf.keras.layers.Conv2D(1, (1, 1))(inputs)  # hanya untuk membuat formatnya\n",
    "model = tf.keras.Model(inputs=inputs, outputs=x)\n",
    "\n",
    "# Konversi ke TFLite\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Simpan file TFLite\n",
    "with open(\"build/csrnet_mobile_B.tflite\", \"wb\") as f:\n",
    "    f.write(tflite_model)\n",
    "\n",
    "print(\"✅ TFLite model saved as csrnet_mobile_B.tflite\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
