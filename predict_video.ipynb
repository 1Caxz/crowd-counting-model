{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5962176b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame 1: Estimated Count = 31.53\n",
      "Frame 2: Estimated Count = 33.67\n",
      "Frame 3: Estimated Count = 33.18\n",
      "Frame 4: Estimated Count = 34.69\n",
      "Frame 5: Estimated Count = 34.21\n",
      "Frame 6: Estimated Count = 33.93\n",
      "Frame 7: Estimated Count = 33.54\n",
      "Frame 8: Estimated Count = 33.48\n",
      "Frame 9: Estimated Count = 33.14\n",
      "Frame 10: Estimated Count = 32.55\n",
      "Frame 11: Estimated Count = 32.73\n",
      "Frame 12: Estimated Count = 32.12\n",
      "Frame 13: Estimated Count = 31.53\n",
      "Frame 14: Estimated Count = 30.00\n",
      "Frame 15: Estimated Count = 28.79\n",
      "Frame 16: Estimated Count = 27.61\n",
      "Frame 17: Estimated Count = 26.72\n",
      "Frame 18: Estimated Count = 26.60\n",
      "Frame 19: Estimated Count = 26.08\n",
      "Frame 20: Estimated Count = 26.54\n",
      "Frame 21: Estimated Count = 27.20\n",
      "Frame 22: Estimated Count = 27.73\n",
      "Frame 23: Estimated Count = 29.26\n",
      "Frame 24: Estimated Count = 30.80\n",
      "Frame 25: Estimated Count = 32.67\n",
      "Frame 26: Estimated Count = 34.18\n",
      "Frame 27: Estimated Count = 35.36\n",
      "Frame 28: Estimated Count = 35.14\n",
      "Frame 29: Estimated Count = 35.65\n",
      "Frame 30: Estimated Count = 35.49\n",
      "Frame 31: Estimated Count = 33.83\n",
      "Frame 32: Estimated Count = 33.37\n",
      "Frame 33: Estimated Count = 32.21\n",
      "Frame 34: Estimated Count = 31.65\n",
      "Frame 35: Estimated Count = 30.85\n",
      "Frame 36: Estimated Count = 30.96\n",
      "Frame 37: Estimated Count = 30.48\n",
      "Frame 38: Estimated Count = 30.80\n",
      "Frame 39: Estimated Count = 30.73\n",
      "Frame 40: Estimated Count = 31.03\n",
      "Frame 41: Estimated Count = 32.15\n",
      "Frame 42: Estimated Count = 32.88\n",
      "Frame 43: Estimated Count = 33.49\n",
      "Frame 44: Estimated Count = 34.45\n",
      "Frame 45: Estimated Count = 35.27\n",
      "Frame 46: Estimated Count = 34.65\n",
      "Frame 47: Estimated Count = 35.00\n",
      "Frame 48: Estimated Count = 34.57\n",
      "Frame 49: Estimated Count = 34.03\n",
      "Frame 50: Estimated Count = 33.17\n",
      "Frame 51: Estimated Count = 32.60\n",
      "Frame 52: Estimated Count = 31.83\n",
      "Frame 53: Estimated Count = 31.20\n",
      "Frame 54: Estimated Count = 29.80\n",
      "Frame 55: Estimated Count = 28.68\n",
      "Frame 56: Estimated Count = 27.98\n",
      "Frame 57: Estimated Count = 27.14\n",
      "Frame 58: Estimated Count = 26.34\n",
      "Frame 59: Estimated Count = 25.99\n",
      "Frame 60: Estimated Count = 26.03\n",
      "Frame 61: Estimated Count = 26.48\n",
      "Frame 62: Estimated Count = 27.16\n",
      "Frame 63: Estimated Count = 27.48\n",
      "Frame 64: Estimated Count = 28.83\n",
      "Frame 65: Estimated Count = 29.74\n",
      "Frame 66: Estimated Count = 30.98\n",
      "Frame 67: Estimated Count = 32.22\n",
      "Frame 68: Estimated Count = 33.43\n",
      "Frame 69: Estimated Count = 35.09\n",
      "Frame 70: Estimated Count = 36.39\n",
      "Frame 71: Estimated Count = 37.02\n",
      "Frame 72: Estimated Count = 36.92\n",
      "Frame 73: Estimated Count = 36.72\n",
      "Frame 74: Estimated Count = 35.31\n",
      "Frame 75: Estimated Count = 34.72\n",
      "Frame 76: Estimated Count = 33.75\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 50\u001b[39m\n\u001b[32m     48\u001b[39m input_tensor = preprocess(frame)\n\u001b[32m     49\u001b[39m interpreter.set_tensor(input_details[\u001b[32m0\u001b[39m][\u001b[33m'\u001b[39m\u001b[33mindex\u001b[39m\u001b[33m'\u001b[39m], input_tensor)\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m \u001b[43minterpreter\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     51\u001b[39m output = interpreter.get_tensor(output_details[\u001b[32m0\u001b[39m][\u001b[33m'\u001b[39m\u001b[33mindex\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m     52\u001b[39m density_map = output.squeeze()  \u001b[38;5;66;03m# shape: (H, W)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Git/crowd-counting-model/venv/lib/python3.12/site-packages/ai_edge_litert/interpreter.py:985\u001b[39m, in \u001b[36mInterpreter.invoke\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    973\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Invoke the interpreter.\u001b[39;00m\n\u001b[32m    974\u001b[39m \n\u001b[32m    975\u001b[39m \u001b[33;03mBe sure to set the input sizes, allocate tensors and fill values before\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    982\u001b[39m \u001b[33;03m  ValueError: When the underlying interpreter fails raise ValueError.\u001b[39;00m\n\u001b[32m    983\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    984\u001b[39m \u001b[38;5;28mself\u001b[39m._ensure_safe()\n\u001b[32m--> \u001b[39m\u001b[32m985\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_interpreter\u001b[49m\u001b[43m.\u001b[49m\u001b[43mInvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "from ai_edge_litert.interpreter import Interpreter\n",
    "\n",
    "# ==== Config ====\n",
    "TFLITE_MODEL_PATH = \"build/csrnet_mobile_B_float16.tflite\"\n",
    "VIDEO_PATH = \"inference/test_videos/crowd_video_test1.mp4\"\n",
    "QUEUE_SIZE = 10  # number of frames for temporal smoothing\n",
    "FRAME_SKIP = 1   # skip every N frames\n",
    "VISUALIZE = False\n",
    "\n",
    "# ==== Load TFLite Model ====\n",
    "interpreter = Interpreter(model_path=TFLITE_MODEL_PATH)\n",
    "interpreter.allocate_tensors()\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "input_shape = input_details[0]['shape']\n",
    "is_channels_first = input_shape[1] == 3\n",
    "\n",
    "# ==== Preprocessing ====\n",
    "def preprocess(frame):\n",
    "    img = cv2.resize(frame, (512, 512))\n",
    "    img = img.astype(np.float32) / 255.0\n",
    "    img = (img - [0.485, 0.456, 0.406]) / [0.229, 0.224, 0.225]\n",
    "    if not is_channels_first:\n",
    "        img = img  # NHWC\n",
    "    else:\n",
    "        img = np.transpose(img, (2, 0, 1))  # CHW\n",
    "    return np.expand_dims(img, axis=0).astype(np.float32)\n",
    "\n",
    "# ==== Spatio-temporal smoothing ====\n",
    "density_queue = deque(maxlen=QUEUE_SIZE)\n",
    "\n",
    "cap = cv2.VideoCapture(VIDEO_PATH)\n",
    "frame_count = 0\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    frame_count += 1\n",
    "    if frame_count % FRAME_SKIP != 0:\n",
    "        continue\n",
    "\n",
    "    input_tensor = preprocess(frame)\n",
    "    interpreter.set_tensor(input_details[0]['index'], input_tensor)\n",
    "    interpreter.invoke()\n",
    "    output = interpreter.get_tensor(output_details[0]['index'])\n",
    "    density_map = output.squeeze()  # shape: (H, W)\n",
    "    density_queue.append(density_map)\n",
    "\n",
    "    # Temporal smoothing\n",
    "    smoothed_density = np.mean(np.stack(density_queue), axis=0)\n",
    "    predicted_count = smoothed_density.sum() - 90\n",
    "\n",
    "    print(f\"Frame {frame_count}: Estimated Count = {predicted_count:.2f}\")\n",
    "\n",
    "    if VISUALIZE:\n",
    "        vis = (smoothed_density / smoothed_density.max() * 255).astype(np.uint8)\n",
    "        vis = cv2.applyColorMap(vis, cv2.COLORMAP_JET)\n",
    "        vis = cv2.resize(vis, (frame.shape[1], frame.shape[0]))\n",
    "        overlay = cv2.addWeighted(frame, 0.6, vis, 0.4, 0)\n",
    "        overlay_rgb = cv2.cvtColor(overlay, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.imshow(overlay_rgb)\n",
    "        plt.title(f\"Frame {frame_count}: Count = {predicted_count:.1f}\")\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "cap.release()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
